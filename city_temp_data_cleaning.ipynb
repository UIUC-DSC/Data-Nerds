{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can look at different datasets and explore them a bit, seeing what the data holds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install a package just do: !pip install [package_name]\n",
    "# !pip install pandas\n",
    "# !pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view multimodel dataset\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "filepath = 'datasets/multimodal-deep-learning-disaster-response-mouzannar (1).zip'\n",
    "\n",
    "# with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('multimodal_data')\n",
    "\n",
    "# shutil.move('multimodal_data', 'datasets/multimodal_data')\n",
    "# os.remove(\"multimodal_data\")\n",
    "\n",
    "##\n",
    "# after looking the files, there are 6 folders with both images and attached text files\n",
    "# does not seem that useful to us at the moment, unless we want to process images which\n",
    "# I don't think we need to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view weather datasets\n",
    "# I gathered from https://scacis.rcc-acis.org/\n",
    "# Product Selection -> daily data listing\n",
    "# Option selection -> start date 3/1/2000 -- 3/1/2023\n",
    "# Station/Area -> los angeles (USC) and san francisco (downtown)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "la_df = pd.read_csv('datasets/weather_data/Los_Angeles_March_2020-2023.csv')\n",
    "sf_df = pd.read_csv('datasets/weather_data/San_Francisco_March_2020-2023.csv')\n",
    "sd_df = pd.read_csv('datasets/weather_data/san_diego.csv')\n",
    "rv_df = pd.read_csv('datasets/weather_data/riverside.csv')\n",
    "\n",
    "dataframes = []\n",
    "dataframes.extend([la_df, sf_df, sd_df, rv_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "for i in dataframes:\n",
    "    cols=[j for j in i.columns if j not in [\"Date\"]]\n",
    "    for col in cols:\n",
    "        i[col]=pd.to_numeric(i[col], errors='coerce')\n",
    "    print(i['Date'].count())\n",
    "\n",
    "places = ['los_angeles','san_fran', 'san_diego', 'riverside']\n",
    "for i in range(len(places)):\n",
    "    dataframes[i]['place']= places[i]\n",
    "\n",
    "# dataframes[0]['place'] = 'Los_Angeles'\n",
    "# dataframes[1]['place'] = 'San_Francisco'\n",
    "# dataframes[2]['place'] = 'San_Diego'\n",
    "# dataframes[3]['place'] = 'Riverside'\n",
    "\n",
    "data = pd.concat(dataframes)\n",
    "# print(data.describe)\n",
    "# data = data[data[' AvgTemperature'].notna()]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see most nan columns\n",
    "# alsolooks like snowfall snow depth, and at obs temp are useless to us\n",
    "print(data.isna().sum())\n",
    "data.drop(columns=[' Snowfall', ' SnowDepth', ' AtObsTemperature'], inplace=True)\n",
    "print(data.isna().sum())\n",
    "data.describe\n",
    "# now remove NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now delete extra NaN rows\n",
    "print(data['place'].value_counts())\n",
    "data1 = data.copy(deep=True)\n",
    "data1 = data1.dropna(subset=[' MaxTemperature', ' MinTemperature', ' AvgTemperature', ' Precipitation'])\n",
    "\n",
    "print(data1.columns)\n",
    "print(data1.loc[:,[' MaxTemperature', ' MinTemperature', ' AvgTemperature', ' Precipitation']])\n",
    "print(data1['place'].value_counts())\n",
    "# print(data[' MaxTemperature'].isna().sum(), data[' MinTemperature'].isna().sum(), data[' AvgTemperature'].isna().sum())\n",
    "print(data1.isna().sum())\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For breakdown of each city, this is what it looks like:\n",
    "# [30333 rows x 3 columns]\n",
    "# san_diego      8483\n",
    "# riverside      8483\n",
    "# los_angeles    8402\n",
    "# san_fran       8402\n",
    "# Name: place, dtype: int64\n",
    "# \n",
    "# when we remove missing values, only riverside contians these missing values\n",
    "# these were: [' MaxTemperature', ' MinTemperature', ' AvgTemperature', ' Precipitation']\n",
    "# Removed missing values:\n",
    "# [29350 rows x 4 columns]\n",
    "# san_fran       8394\n",
    "# los_angeles    8119\n",
    "# san_diego      7876\n",
    "# riverside      4961\n",
    "# Name: place, dtype: int64\n",
    "\n",
    "data1.isna().sum()\n",
    "# NO more missing!\n",
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# save as csv\n",
    "data1.to_csv('datasets/outputs/city_temp.csv', header=['date', 'max_temp', ' min_temp', 'mean_temp', 'rain', 'city'])\n",
    "# with open('city_temp.csv', 'w') as csv_file:\n",
    "#     data1.to_csv(path_or_buf=csv_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED HELP WITH THIS PART!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see some histograms on some columns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates\n",
    "\n",
    "# need to convert date to datetime to do this\n",
    "la_df['Date'] = pd.to_datetime(la_df['Date'])\n",
    "\n",
    "# plt.hist(la_df[' AvgTemperature'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(la_df['Date'], la_df[' AvgTemperature'])\n",
    "ax.set_ylim([40,85])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did some exploring on the data, I want ot look more at the sanfran data too. We can get more data around the USA just let me know.\\\n",
    "Also, the data you guys try and find let's see if we can concatonate it with my data and we can come up with a correlation of some sort.\\\n",
    "When you guys find a dataset, just put  it into the datasets folder\\\n",
    "If you guys want to look at more cool things with the above data go ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "722e8765311bc783d01f25953f3c9b79780613b39edb0509b118f4c4b0314537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
